{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3bd6ab",
   "metadata": {},
   "source": [
    "# Capstone Project: Predicting the need for hospital admittance using machine learning\n",
    "### (Notebook 2 of 2)\n",
    "\n",
    "By: Yash Nagpaul\n",
    "\n",
    "*(Data Science Diploma Candidate, BrainStation)*\n",
    "\n",
    "## Table of Contents:\n",
    "1. <a href=\"#Note:\">Introductory note</a>\n",
    "2. <a href=\"#Part-3-—-Modelling\">Part 3 — Modelling</a>\n",
    "\n",
    "---\n",
    "#### Note:\n",
    "This is the second of two Jupyter notebooks used in this project. In this notebook, we will be modelling using the cleaned and scaled datasets created in the first notebook.\n",
    "\n",
    "---\n",
    "### Part 3 — Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55dc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ecf6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.55037225e-02, -9.23507447e-01, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03],\n",
       "       [-1.55037225e-02,  8.88801104e-01, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03],\n",
       "       [-1.55037225e-02,  6.92875855e-01, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03],\n",
       "       ...,\n",
       "       [ 1.13714702e+00, -7.27582198e-01, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03],\n",
       "       [ 2.28979776e+00, -1.26637663e+00, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03],\n",
       "       [-1.16815446e+00,  1.23167029e+00, -4.64896751e-02, ...,\n",
       "        -1.91282758e-02, -1.05535639e-01, -2.11198305e-03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.loadtxt('./X_train.txt', delimiter=',')\n",
    "X_test = np.loadtxt('./X_test.txt', delimiter=',')\n",
    "y_train = np.loadtxt('./y_train.txt', delimiter=',')\n",
    "y_test = np.loadtxt('./y_test.txt', delimiter=',')\n",
    "\n",
    "# Sanity check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73c697",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Let's begin with one of the simplest classification models: A Logistic Regression.\n",
    "    - Briefly described, a logistic regression fits a logistic function on the input variables and outputs the probability of a binary target variable.\n",
    "- First, we will run a simple Logistic Regression without worrying about any hyperparameter optimization.\n",
    "- That way, we will quickly get a baseline model accuracy to work with and improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb6b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with training data: 86.07 %\n",
      "Model accuracy with test data: 86.04 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "logreg = LogisticRegression(random_state=7)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "train_acc = logreg.score(X_train, y_train)\n",
    "test_acc = logreg.score(X_test, y_test)\n",
    "\n",
    "print('Model accuracy with training data:', round(train_acc*100,2),\"%\")\n",
    "print('Model accuracy with test data:', round(test_acc*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b3e00",
   "metadata": {},
   "source": [
    "#### DISCUSSION:\n",
    "- Our first model is about 86% accurate on unseen data!\n",
    "- That's not bad at all since it's significantly better than what we would get if we simply guessed every disposition as '0' (since 70% of all dispositions in our dataset are '0').\n",
    "- It is important to note that we dropped over 300 columns since they were missing more than 10% of the values.\n",
    "- Yet, we are able to achieve an accuracy that is almost at par with the original study.\n",
    "- Let us save the model that we just trained into a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49959589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "logreg_pkl_file = 'disposition_logreg.pkl'\n",
    "\n",
    "with open(logreg_pkl_file, 'wb') as file:\n",
    "    pickle.dump(logreg, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922b6af",
   "metadata": {},
   "source": [
    "- Let's try to improve this result by optimizing the regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb72d6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': np.logspace(-3, 7, 11),\n",
    "    'penalty': ['l2']\n",
    "    # The 'lbfgs' is the fastest solver for large datasets.\n",
    "    # However, it doen't work with L1 penalty.\n",
    "    # We will address this issue later.\n",
    "}\n",
    "\n",
    "# GridSearchCV allows us to search over multiple params in a model\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "logreg_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44c6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with training data: 86.07 %\n",
      "Model accuracy with test data: 86.03 %\n"
     ]
    }
   ],
   "source": [
    "logreg2 = LogisticRegression(random_state=7, C= 0.1, penalty= 'l2')\n",
    "logreg2.fit(X_train, y_train)\n",
    "\n",
    "train_acc = logreg2.score(X_train, y_train)\n",
    "test_acc = logreg2.score(X_test, y_test)\n",
    "\n",
    "print('Model accuracy with training data:', round(train_acc*100,2),\"%\")\n",
    "print('Model accuracy with test data:', round(test_acc*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd383829",
   "metadata": {},
   "source": [
    "- Very well. We can see that even though the accuracies dropped slightly, the train and test accuracies are equal now.\n",
    "- This means that with these hyperparameters, our model has found a perfect balance between bias and overfitting.\n",
    "- Now, we want to try various methods to account for any multicoliearity in the dataset.\n",
    "- I am keen on fitting a logreg model that works with L1 penalty. Therefore, I will set the *solver* to be 'saga' (since this is a large dataset)\n",
    "- And then, let us repeat the process of finding the best hyperparameters through a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33497e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELLS 11 and 12 AGAIN BEFORE PROCEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125743a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASICALLY GRID SEARCH AND EVERYTHING PAST THAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cdc62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
